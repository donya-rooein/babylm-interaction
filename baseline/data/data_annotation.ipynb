{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932bacd9",
   "metadata": {},
   "source": [
    "# Preliminary Data Annotation\n",
    "\n",
    "We want positive/negative examples annotated with a series of linguistic metrics (coherence, fluency) both at the utterance level and at the dialogue level (< 5 turns). \n",
    "\n",
    "- Positive examples will be taken from the [BabyLM (Switchboard)](https://huggingface.co/datasets/hhoangphuoc/switchboard) dataset.\n",
    "- Negative examples will be taken from BabyLlama outputs.\n",
    "\n",
    "Corpus size: no more than 20 million tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f704407",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee1b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40262fe1",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c8a70",
   "metadata": {},
   "source": [
    "### 2.1 BabyLM (Switchboard) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4f33c",
   "metadata": {},
   "source": [
    "#### 2.1.1 Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.00k/2.00k [00:00<00:00, 2.19MB/s]\n",
      "Downloading data: 100%|██████████| 378M/378M [00:38<00:00, 9.84MB/s] \n",
      "Downloading data: 100%|██████████| 383M/383M [00:51<00:00, 7.40MB/s] \n",
      "Downloading data: 100%|██████████| 389M/389M [00:48<00:00, 8.09MB/s] \n",
      "Downloading data: 100%|██████████| 401M/401M [00:42<00:00, 9.36MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:48<00:00, 8.16MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:52<00:00, 7.32MB/s] \n",
      "Downloading data: 100%|██████████| 391M/391M [00:42<00:00, 9.27MB/s] \n",
      "Downloading data: 100%|██████████| 390M/390M [00:39<00:00, 9.91MB/s] \n",
      "Downloading data: 100%|██████████| 387M/387M [00:40<00:00, 9.67MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:45<00:00, 8.72MB/s] \n",
      "Downloading data: 100%|██████████| 396M/396M [01:02<00:00, 6.35MB/s] \n",
      "Downloading data: 100%|██████████| 397M/397M [00:44<00:00, 8.98MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:36<00:00, 10.6MB/s] \n",
      "Downloading data: 100%|██████████| 404M/404M [00:39<00:00, 10.2MB/s] \n",
      "Downloading data: 100%|██████████| 403M/403M [00:37<00:00, 10.6MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:35<00:00, 11.0MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:34<00:00, 11.3MB/s] \n",
      "Downloading data: 100%|██████████| 389M/389M [00:35<00:00, 11.0MB/s] \n",
      "Downloading data: 100%|██████████| 400M/400M [00:37<00:00, 10.6MB/s] \n",
      "Downloading data: 100%|██████████| 386M/386M [00:37<00:00, 10.4MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:44<00:00, 8.72MB/s] \n",
      "Downloading data: 100%|██████████| 401M/401M [00:55<00:00, 7.27MB/s] \n",
      "Downloading data: 100%|██████████| 397M/397M [00:57<00:00, 6.87MB/s] \n",
      "Downloading data: 100%|██████████| 390M/390M [00:51<00:00, 7.52MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:56<00:00, 6.94MB/s] \n",
      "Downloading data: 100%|██████████| 395M/395M [00:36<00:00, 10.9MB/s] \n",
      "Downloading data: 100%|██████████| 398M/398M [00:47<00:00, 8.36MB/s] \n",
      "Downloading data: 100%|██████████| 387M/387M [00:37<00:00, 10.3MB/s] \n",
      "Downloading data: 100%|██████████| 383M/383M [00:42<00:00, 8.98MB/s] \n",
      "Downloading data: 100%|██████████| 399M/399M [00:52<00:00, 7.55MB/s] \n",
      "Downloading data: 100%|██████████| 379M/379M [00:59<00:00, 6.39MB/s] \n",
      "Downloading data: 100%|██████████| 393M/393M [00:51<00:00, 7.62MB/s] \n",
      "Downloading data: 100%|██████████| 403M/403M [00:42<00:00, 9.52MB/s] \n",
      "Downloading data: 100%|██████████| 402M/402M [00:59<00:00, 6.78MB/s] \n",
      "Downloading data: 100%|██████████| 391M/391M [00:40<00:00, 9.56MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:40<00:00, 9.70MB/s] \n",
      "Downloading data: 100%|██████████| 391M/391M [00:39<00:00, 9.81MB/s] \n",
      "Downloading data: 100%|██████████| 403M/403M [00:42<00:00, 9.50MB/s] \n",
      "Downloading data: 100%|██████████| 402M/402M [00:54<00:00, 7.33MB/s] \n",
      "Downloading data: 100%|██████████| 389M/389M [00:40<00:00, 9.61MB/s] \n",
      "Downloading data: 100%|██████████| 380M/380M [00:33<00:00, 11.2MB/s] \n",
      "Downloading data: 100%|██████████| 393M/393M [00:51<00:00, 7.58MB/s] \n",
      "Downloading data: 100%|██████████| 376M/376M [00:37<00:00, 9.97MB/s] \n",
      "Downloading data: 100%|██████████| 388M/388M [00:42<00:00, 9.04MB/s] \n",
      "Downloading data: 100%|██████████| 398M/398M [00:47<00:00, 8.42MB/s] \n",
      "Downloading data: 100%|██████████| 395M/395M [00:40<00:00, 9.80MB/s] \n",
      "Downloading data: 100%|██████████| 397M/397M [00:45<00:00, 8.66MB/s] \n",
      "Downloading data: 100%|██████████| 390M/390M [00:41<00:00, 9.34MB/s] \n",
      "Downloading data: 100%|██████████| 395M/395M [00:44<00:00, 8.86MB/s] \n",
      "Downloading data: 100%|██████████| 399M/399M [00:40<00:00, 9.77MB/s] \n",
      "Downloading data: 100%|██████████| 387M/387M [00:49<00:00, 7.89MB/s] \n",
      "Downloading data: 100%|██████████| 382M/382M [00:40<00:00, 9.48MB/s] \n",
      "Downloading data: 100%|██████████| 386M/386M [00:52<00:00, 7.32MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:33<00:00, 11.5MB/s] \n",
      "Downloading data: 100%|██████████| 392M/392M [00:41<00:00, 9.41MB/s] \n",
      "Downloading data: 100%|██████████| 397M/397M [00:47<00:00, 8.38MB/s] \n",
      "Downloading data: 100%|██████████| 386M/386M [00:57<00:00, 6.67MB/s] \n",
      "Downloading data: 100%|██████████| 393M/393M [00:49<00:00, 7.92MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:48<00:00, 7.96MB/s] \n",
      "Downloading data: 100%|██████████| 394M/394M [00:51<00:00, 7.67MB/s] \n",
      "Downloading data: 100%|██████████| 385M/385M [00:50<00:00, 7.68MB/s] \n",
      "Downloading data: 100%|██████████| 399M/399M [00:50<00:00, 7.94MB/s] \n",
      "Downloading data: 100%|██████████| 387M/387M [00:41<00:00, 9.29MB/s] \n",
      "Downloading data: 100%|██████████| 393M/393M [00:33<00:00, 11.5MB/s] \n",
      "Downloading data: 100%|██████████| 396M/396M [00:57<00:00, 6.90MB/s] \n",
      "Downloading data:   5%|▌         | 21.0M/396M [00:02<00:40, 9.32MB/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"hhoangphuoc/switchboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0fc91",
   "metadata": {},
   "source": [
    "#### 2.1.2 Selecting positive examples\n",
    "\n",
    "What's our selection criteria? Are we basing it on the metrics we annotate (high vs. low scores) or what we think looks good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c67f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "971a22b9",
   "metadata": {},
   "source": [
    "### 2.2 Prompt BabyLlama Model\n",
    "\n",
    "What will the prompts be? Are we simulating dialogues with a teacher? Could these come from our positive examples in the BabyLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"timinar/baby-llama-58m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"timinar/baby-llama-58m\")\n",
    "\n",
    "# Prompt\n",
    "input_text = ...\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate continuation\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print result\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80e595",
   "metadata": {},
   "source": [
    "## 3. Metrics\n",
    "\n",
    "We discussed Fluency and Coherence as the two important things we want to annotate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc9d0a",
   "metadata": {},
   "source": [
    "### 3.1 Fluency \n",
    "\n",
    "Fluency \"measures the quality of individual sentences, are they grammatically correct, non-repetitive, and in accord with common English usage, with clear meanings\" ([Hu et al., 2024](https://arxiv.org/html/2402.12055v2)).\n",
    "\n",
    "From this definition, we decompose Fluency into Grammaticality and Conciseness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b50432",
   "metadata": {},
   "source": [
    "#### 3.1.1 Grammaticality\n",
    "\n",
    "Might consider using [ERRANT](https://github.com/chrisjbryant/errant) for error detection and classification. \n",
    "\n",
    "Alternatively, below is an example using SpaCy's contextual [spell-checker](https://pypi.org/project/contextualSpellCheck/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Income was $9.4 million compared to the prior year of $2.7 million.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Income was $9.4 milion comare to the prior year of $2.7 milion.')\n",
    "\n",
    "print(doc._.performed_spellCheck) #Should be True\n",
    "print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef0eb5",
   "metadata": {},
   "source": [
    "#### 3.1.2 Conciseness\n",
    "\n",
    "We could use the Conciseness metric define by [Cao and Zhuge (2022)](https://www.sciencedirect.com/science/article/pii/S0957417422010491) which \"considers both the repetition of representations within each sentence and the similarity between sentences as redundancy contained in the summary, and adds the location of sentence to the calculation of redundancy for allowing the existence of some similarity between sentences that are far apart.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7d522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "429ed374",
   "metadata": {},
   "source": [
    "### 3.2 Coherence/cohesion\n",
    "\n",
    "TAACO is a tool for the automatic analysis tool of local and global text cohesion ([Crossley et al., 2016](https://link.springer.com/article/10.3758/s13428-015-0651-7)).\n",
    "\n",
    "Below are the different TAACO measures that `test_shiva_w_source.py` retrieves for you with their column indices in the output csv file. If we do not care about source-related measure (e.g., dialogue-level, between teacher and student turns), use `test_shiva.py` instead.\n",
    "\n",
    "- ttr related metrics:  columns[1:16]\n",
    "- Overlaps in Adjacent Sentences: columns[16:70]\n",
    "- Overlaps in Adjacent Paragraphs: columns[70:124]\n",
    "- Synonym overlap in adjacent sents: columns[124:126]\n",
    "- Synonym overlap in adjacent pars: columns[126:128]\n",
    "- LSA: columns[128:132]\n",
    "- LDA: columns[132:136]\n",
    "- word2vec: columns[136:140]\n",
    "- Others: columns[140:170]    only positive casual and positive sth are not bad\n",
    "- Topic relevance, source similarity, mag_news keywords columns[170:192]???\n",
    "- source similarity, LSA, LDA, W2V columns[192:196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59300802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5d6f848",
   "metadata": {},
   "source": [
    "## 4. Annotate\n",
    "\n",
    "Below we annotate both the positive examples from the BabyLM (SwitchBoard) corpus and the negative examples generated using BabyLlama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e84f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
