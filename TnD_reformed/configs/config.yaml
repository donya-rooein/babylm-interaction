# General configuration for TnD training

# Model settings
teacher_model: "gpt2"
student_model: "gpt2"
reward_model: "bert-base-uncased"

# Training settings
num_epochs: 3
batch_size: 32
learning_rate: 5e-5
warmup_steps: 500
weight_decay: 0.01
max_length: 512
gradient_accumulation_steps: 4

# PPO settings
entropy_coeff: 0.1
loss_scaling: 1.0

# Dataset settings
dataset_path: "data/tnd"
output_dir: "outputs/tnd"
logging_dir: "logs/tnd"

# Evaluation settings
eval_steps: 100
save_steps: 1000
logging_steps: 100 